# app.py
import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from deep_translator import GoogleTranslator  # ğŸ‘ˆ Nuevo

# ----------------------------
# 1ï¸âƒ£ Crear/actualizar embeddings
# ----------------------------
def actualiza_embedding(pdf_path: str, persist_dir: str):
    st.info("ğŸ“„ Cargando PDF y creando embeddings...")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)
    chunked_docs = text_splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    vectordb = Chroma.from_documents(
        chunked_docs,
        embedding=embeddings,
        persist_directory=persist_dir
    )
    st.success("âœ… Embeddings creados y guardados en ./chroma_db")
    return vectordb


# ----------------------------
# 2ï¸âƒ£ Interfaz Streamlit
# ----------------------------
st.set_page_config(page_title="Mesa de Ayuda IA (Local)", page_icon="ğŸ¤–")
st.title("ğŸ¤– Mesa de Ayuda IA (Local)")
st.write("Agente de asistencia tÃ©cnica que responde con base en el contenido del documento cargado.")

pdf_path = "data/manual.pdf"
persist_dir = "./chroma_db"

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
if os.path.exists(persist_dir):
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
else:
    vectordb = None
    st.warning("âš ï¸ No hay base de datos vectorial. Debes actualizar embeddings primero.")

if st.button("ğŸ”„ Actualizar Embeddings"):
    if os.path.exists(pdf_path):
        vectordb = actualiza_embedding(pdf_path, persist_dir)
    else:
        st.error("âŒ No se encontrÃ³ el PDF en data/manual.pdf")


# ----------------------------
# 3ï¸âƒ£ Configurar LLM local desde LM Studio
# ----------------------------
llm = ChatOpenAI(
    model="mistral-7b-instruct-v0.2.Q4_K_M",
    openai_api_base="http://localhost:1234/v1",
    api_key="not-needed",
    temperature=0.1,
    max_tokens=1200
)

prompt_template = """
Eres un agente de mesa de ayuda especializado en software empresarial.
Tu tarea es responder SIEMPRE en ESPAÃ‘OL, sin excepciones.
Debes redactar respuestas claras, extensas y con explicaciones tÃ©cnicas.
Usa ÃšNICAMENTE la informaciÃ³n del manual proporcionado.
Si la informaciÃ³n no estÃ¡ en el contexto, responde exactamente:
"No encuentro esa informaciÃ³n en el documento."

Pregunta del usuario: {input}
Contexto extraÃ­do del manual: {context}
"""

qa_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template(prompt_template)
)


# ----------------------------
# 4ï¸âƒ£ BÃºsqueda y respuesta con barra de estado
# ----------------------------
pregunta = st.text_area("âœï¸ Escribe tu pregunta aquÃ­:")

if st.button("ğŸ“¨ Enviar"):
    if not vectordb:
        st.error("âš ï¸ Primero genera embeddings con el botÃ³n 'ğŸ”„ Actualizar Embeddings'.")
    elif pregunta:
        progress_bar = st.progress(0, text="â³ Procesando tu pregunta...")

        progress_bar.progress(25, text="ğŸ” Buscando informaciÃ³n relevante en el manual...")
        resultados = vectordb.similarity_search(pregunta, k=8)
        contexto = " ".join([doc.page_content for doc in resultados])

        progress_bar.progress(60, text="âš™ï¸ Generando respuesta detallada con el modelo...")
        respuesta = qa_chain.invoke({"input": pregunta, "context": contexto})

        # ğŸ‘‡ Traducir siempre la salida al espaÃ±ol
        respuesta_final = GoogleTranslator(source="auto", target="es").translate(respuesta["text"])

        progress_bar.progress(100, text="âœ… Respuesta lista")

        st.subheader("ğŸ“˜ Respuesta:")
        st.write(respuesta_final)
    else:
        st.warning("âš ï¸ Por favor ingresa una pregunta antes de enviar.")
